{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ca201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – metadata\n",
    "# Direct Pandas Pipeline – chunked CSV → Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c8ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 – imports & config\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "CSV1 = Path(\"../data/raw/dataset1_final.csv\")\n",
    "CSV2 = Path(\"../data/raw/dataset2_final.csv\")\n",
    "CHUNK = 50_000\n",
    "PARQUET_DIR = Path(\"../data/parquet\")\n",
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbd66df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>medium</th>\n",
       "      <th>purchase_revenue</th>\n",
       "      <th>total_item_quantity</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>eventTimestamp</th>\n",
       "      <th>gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>page_type</th>\n",
       "      <th>income_group</th>\n",
       "      <th>page_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.789251e+09</td>\n",
       "      <td>session_start</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Poquoson</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 10:21:57.850268</td>\n",
       "      <td>male</td>\n",
       "      <td>35-44</td>\n",
       "      <td>homepage</td>\n",
       "      <td>Top 10%</td>\n",
       "      <td>https://demo.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.789251e+09</td>\n",
       "      <td>page_view</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Poquoson</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 10:21:57.850268</td>\n",
       "      <td>female</td>\n",
       "      <td>above 64</td>\n",
       "      <td>homepage</td>\n",
       "      <td>below 50%</td>\n",
       "      <td>https://demo.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.788384e+09</td>\n",
       "      <td>session_start</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Carthage</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 12:38:06.968220</td>\n",
       "      <td>male</td>\n",
       "      <td>45-54</td>\n",
       "      <td>collections</td>\n",
       "      <td>11-20%</td>\n",
       "      <td>https://demo.com/collections/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.788384e+09</td>\n",
       "      <td>page_view</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Carthage</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 12:38:06.968220</td>\n",
       "      <td>male</td>\n",
       "      <td>45-54</td>\n",
       "      <td>collections</td>\n",
       "      <td>11-20%</td>\n",
       "      <td>https://demo.com/collections/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.198796e+09</td>\n",
       "      <td>page_view</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>United States</td>\n",
       "      <td>(direct)</td>\n",
       "      <td>(none)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 14:20:32.933828</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34</td>\n",
       "      <td>products</td>\n",
       "      <td>Top 10%</td>\n",
       "      <td>https://demo.com/products/ITEM377/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_pseudo_id     event_name category      city    region        country  \\\n",
       "0    1.789251e+09  session_start   mobile  Poquoson  Virginia  United States   \n",
       "1    1.789251e+09      page_view   mobile  Poquoson  Virginia  United States   \n",
       "2    1.788384e+09  session_start   mobile  Carthage  New York  United States   \n",
       "3    1.788384e+09      page_view   mobile  Carthage  New York  United States   \n",
       "4    1.198796e+09      page_view   mobile   Phoenix   Arizona  United States   \n",
       "\n",
       "     source      medium  purchase_revenue  total_item_quantity  \\\n",
       "0  Facebook  PaidSocial               NaN                  NaN   \n",
       "1  Facebook  PaidSocial               NaN                  NaN   \n",
       "2  Facebook  PaidSocial               NaN                  NaN   \n",
       "3  Facebook  PaidSocial               NaN                  NaN   \n",
       "4  (direct)      (none)               NaN                  NaN   \n",
       "\n",
       "   transaction_id   eventDate              eventTimestamp  gender       Age  \\\n",
       "0             NaN  2025-05-13  2025-05-13 10:21:57.850268    male     35-44   \n",
       "1             NaN  2025-05-13  2025-05-13 10:21:57.850268  female  above 64   \n",
       "2             NaN  2025-05-13  2025-05-13 12:38:06.968220    male     45-54   \n",
       "3             NaN  2025-05-13  2025-05-13 12:38:06.968220    male     45-54   \n",
       "4             NaN  2025-05-13  2025-05-13 14:20:32.933828    male     25-34   \n",
       "\n",
       "     page_type income_group                           page_path  \n",
       "0     homepage      Top 10%                   https://demo.com/  \n",
       "1     homepage    below 50%                   https://demo.com/  \n",
       "2  collections       11-20%       https://demo.com/collections/  \n",
       "3  collections       11-20%       https://demo.com/collections/  \n",
       "4     products      Top 10%  https://demo.com/products/ITEM377/  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(CSV1, nrows=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb586b",
   "metadata": {},
   "source": [
    "chunksize:        50 000\n",
    "parquet_engine:   pyarrow\n",
    "user_pseudo_id:   forced string  (avoid ArrowInvalid on mixed types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6b52f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(src: Path, dst_dir: Path, chunksize: int = 50_000):\n",
    "    \"\"\"Read a large CSV in chunks and write each chunk to Parquet.\"\"\"\n",
    "    import time, pandas as pd\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ↓ Explicit dtype prevents mixed-type surprises\n",
    "    explicit = {\"user_pseudo_id\": \"string\"}      # keep IDs as strings\n",
    "    for i, chunk in enumerate(\n",
    "        pd.read_csv(src,\n",
    "                    chunksize=chunksize,\n",
    "                    dtype=explicit,          # <- key change\n",
    "                    low_memory=False)\n",
    "    ):\n",
    "        # (optional) drop the .000 / scientific notation if you prefer integers\n",
    "        # chunk[\"user_pseudo_id\"] = (\n",
    "        #     pd.to_numeric(chunk[\"user_pseudo_id\"], errors=\"coerce\")\n",
    "        #       .astype(\"Int64\")  # nullable int\n",
    "        # )\n",
    "\n",
    "        outfile = dst_dir / f\"{src.stem}_part{i:03d}.parquet\"\n",
    "        chunk.to_parquet(outfile, engine=\"pyarrow\", index=False)\n",
    "        print(f\"[{i:03d}] wrote {outfile.name}  rows={len(chunk):,}\")\n",
    "\n",
    "    print(\"✅ done in\", round(time.time() - t0, 1), \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc378edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] wrote dataset1_final_part000.parquet  rows=50,000\n",
      "[001] wrote dataset1_final_part001.parquet  rows=50,000\n",
      "[002] wrote dataset1_final_part002.parquet  rows=50,000\n",
      "[003] wrote dataset1_final_part003.parquet  rows=50,000\n",
      "[004] wrote dataset1_final_part004.parquet  rows=50,000\n",
      "[005] wrote dataset1_final_part005.parquet  rows=50,000\n",
      "[006] wrote dataset1_final_part006.parquet  rows=50,000\n",
      "[007] wrote dataset1_final_part007.parquet  rows=50,000\n",
      "[008] wrote dataset1_final_part008.parquet  rows=50,000\n",
      "[009] wrote dataset1_final_part009.parquet  rows=50,000\n",
      "[010] wrote dataset1_final_part010.parquet  rows=50,000\n",
      "[011] wrote dataset1_final_part011.parquet  rows=50,000\n",
      "[012] wrote dataset1_final_part012.parquet  rows=50,000\n",
      "[013] wrote dataset1_final_part013.parquet  rows=50,000\n",
      "[014] wrote dataset1_final_part014.parquet  rows=50,000\n",
      "[015] wrote dataset1_final_part015.parquet  rows=50,000\n",
      "[016] wrote dataset1_final_part016.parquet  rows=50,000\n",
      "[017] wrote dataset1_final_part017.parquet  rows=50,000\n",
      "[018] wrote dataset1_final_part018.parquet  rows=50,000\n",
      "[019] wrote dataset1_final_part019.parquet  rows=50,000\n",
      "[020] wrote dataset1_final_part020.parquet  rows=50,000\n",
      "[021] wrote dataset1_final_part021.parquet  rows=50,000\n",
      "[022] wrote dataset1_final_part022.parquet  rows=50,000\n",
      "[023] wrote dataset1_final_part023.parquet  rows=50,000\n",
      "[024] wrote dataset1_final_part024.parquet  rows=50,000\n",
      "[025] wrote dataset1_final_part025.parquet  rows=50,000\n",
      "[026] wrote dataset1_final_part026.parquet  rows=50,000\n",
      "[027] wrote dataset1_final_part027.parquet  rows=50,000\n",
      "[028] wrote dataset1_final_part028.parquet  rows=50,000\n",
      "[029] wrote dataset1_final_part029.parquet  rows=50,000\n",
      "[030] wrote dataset1_final_part030.parquet  rows=50,000\n",
      "[031] wrote dataset1_final_part031.parquet  rows=50,000\n",
      "[032] wrote dataset1_final_part032.parquet  rows=50,000\n",
      "[033] wrote dataset1_final_part033.parquet  rows=50,000\n",
      "[034] wrote dataset1_final_part034.parquet  rows=50,000\n",
      "[035] wrote dataset1_final_part035.parquet  rows=50,000\n",
      "[036] wrote dataset1_final_part036.parquet  rows=50,000\n",
      "[037] wrote dataset1_final_part037.parquet  rows=50,000\n",
      "[038] wrote dataset1_final_part038.parquet  rows=50,000\n",
      "[039] wrote dataset1_final_part039.parquet  rows=50,000\n",
      "[040] wrote dataset1_final_part040.parquet  rows=50,000\n",
      "[041] wrote dataset1_final_part041.parquet  rows=50,000\n",
      "[042] wrote dataset1_final_part042.parquet  rows=50,000\n",
      "[043] wrote dataset1_final_part043.parquet  rows=50,000\n",
      "[044] wrote dataset1_final_part044.parquet  rows=50,000\n",
      "[045] wrote dataset1_final_part045.parquet  rows=50,000\n",
      "[046] wrote dataset1_final_part046.parquet  rows=50,000\n",
      "[047] wrote dataset1_final_part047.parquet  rows=50,000\n",
      "[048] wrote dataset1_final_part048.parquet  rows=50,000\n",
      "[049] wrote dataset1_final_part049.parquet  rows=50,000\n",
      "[050] wrote dataset1_final_part050.parquet  rows=50,000\n",
      "[051] wrote dataset1_final_part051.parquet  rows=50,000\n",
      "[052] wrote dataset1_final_part052.parquet  rows=50,000\n",
      "[053] wrote dataset1_final_part053.parquet  rows=50,000\n",
      "[054] wrote dataset1_final_part054.parquet  rows=50,000\n",
      "[055] wrote dataset1_final_part055.parquet  rows=50,000\n",
      "[056] wrote dataset1_final_part056.parquet  rows=50,000\n",
      "[057] wrote dataset1_final_part057.parquet  rows=50,000\n",
      "[058] wrote dataset1_final_part058.parquet  rows=50,000\n",
      "[059] wrote dataset1_final_part059.parquet  rows=50,000\n",
      "[060] wrote dataset1_final_part060.parquet  rows=50,000\n",
      "[061] wrote dataset1_final_part061.parquet  rows=50,000\n",
      "[062] wrote dataset1_final_part062.parquet  rows=50,000\n",
      "[063] wrote dataset1_final_part063.parquet  rows=50,000\n",
      "[064] wrote dataset1_final_part064.parquet  rows=50,000\n",
      "[065] wrote dataset1_final_part065.parquet  rows=50,000\n",
      "[066] wrote dataset1_final_part066.parquet  rows=50,000\n",
      "[067] wrote dataset1_final_part067.parquet  rows=50,000\n",
      "[068] wrote dataset1_final_part068.parquet  rows=50,000\n",
      "[069] wrote dataset1_final_part069.parquet  rows=50,000\n",
      "[070] wrote dataset1_final_part070.parquet  rows=50,000\n",
      "[071] wrote dataset1_final_part071.parquet  rows=50,000\n",
      "[072] wrote dataset1_final_part072.parquet  rows=50,000\n",
      "[073] wrote dataset1_final_part073.parquet  rows=50,000\n",
      "[074] wrote dataset1_final_part074.parquet  rows=50,000\n",
      "[075] wrote dataset1_final_part075.parquet  rows=50,000\n",
      "[076] wrote dataset1_final_part076.parquet  rows=50,000\n",
      "[077] wrote dataset1_final_part077.parquet  rows=50,000\n",
      "[078] wrote dataset1_final_part078.parquet  rows=50,000\n",
      "[079] wrote dataset1_final_part079.parquet  rows=50,000\n",
      "[080] wrote dataset1_final_part080.parquet  rows=50,000\n",
      "[081] wrote dataset1_final_part081.parquet  rows=50,000\n",
      "[082] wrote dataset1_final_part082.parquet  rows=50,000\n",
      "[083] wrote dataset1_final_part083.parquet  rows=50,000\n",
      "[084] wrote dataset1_final_part084.parquet  rows=50,000\n",
      "[085] wrote dataset1_final_part085.parquet  rows=50,000\n",
      "[086] wrote dataset1_final_part086.parquet  rows=50,000\n",
      "[087] wrote dataset1_final_part087.parquet  rows=50,000\n",
      "[088] wrote dataset1_final_part088.parquet  rows=50,000\n",
      "[089] wrote dataset1_final_part089.parquet  rows=50,000\n",
      "[090] wrote dataset1_final_part090.parquet  rows=50,000\n",
      "[091] wrote dataset1_final_part091.parquet  rows=50,000\n",
      "[092] wrote dataset1_final_part092.parquet  rows=50,000\n",
      "[093] wrote dataset1_final_part093.parquet  rows=50,000\n",
      "[094] wrote dataset1_final_part094.parquet  rows=50,000\n",
      "[095] wrote dataset1_final_part095.parquet  rows=50,000\n",
      "[096] wrote dataset1_final_part096.parquet  rows=50,000\n",
      "[097] wrote dataset1_final_part097.parquet  rows=50,000\n",
      "[098] wrote dataset1_final_part098.parquet  rows=50,000\n",
      "[099] wrote dataset1_final_part099.parquet  rows=50,000\n",
      "[100] wrote dataset1_final_part100.parquet  rows=50,000\n",
      "[101] wrote dataset1_final_part101.parquet  rows=50,000\n",
      "[102] wrote dataset1_final_part102.parquet  rows=50,000\n",
      "[103] wrote dataset1_final_part103.parquet  rows=50,000\n",
      "[104] wrote dataset1_final_part104.parquet  rows=50,000\n",
      "[105] wrote dataset1_final_part105.parquet  rows=50,000\n",
      "[106] wrote dataset1_final_part106.parquet  rows=50,000\n",
      "[107] wrote dataset1_final_part107.parquet  rows=50,000\n",
      "[108] wrote dataset1_final_part108.parquet  rows=50,000\n",
      "[109] wrote dataset1_final_part109.parquet  rows=50,000\n",
      "[110] wrote dataset1_final_part110.parquet  rows=50,000\n",
      "[111] wrote dataset1_final_part111.parquet  rows=50,000\n",
      "[112] wrote dataset1_final_part112.parquet  rows=50,000\n",
      "[113] wrote dataset1_final_part113.parquet  rows=50,000\n",
      "[114] wrote dataset1_final_part114.parquet  rows=50,000\n",
      "[115] wrote dataset1_final_part115.parquet  rows=50,000\n",
      "[116] wrote dataset1_final_part116.parquet  rows=50,000\n",
      "[117] wrote dataset1_final_part117.parquet  rows=50,000\n",
      "[118] wrote dataset1_final_part118.parquet  rows=50,000\n",
      "[119] wrote dataset1_final_part119.parquet  rows=50,000\n",
      "[120] wrote dataset1_final_part120.parquet  rows=50,000\n",
      "[121] wrote dataset1_final_part121.parquet  rows=50,000\n",
      "[122] wrote dataset1_final_part122.parquet  rows=50,000\n",
      "[123] wrote dataset1_final_part123.parquet  rows=50,000\n",
      "[124] wrote dataset1_final_part124.parquet  rows=50,000\n",
      "[125] wrote dataset1_final_part125.parquet  rows=50,000\n",
      "[126] wrote dataset1_final_part126.parquet  rows=50,000\n",
      "[127] wrote dataset1_final_part127.parquet  rows=50,000\n",
      "[128] wrote dataset1_final_part128.parquet  rows=50,000\n",
      "[129] wrote dataset1_final_part129.parquet  rows=50,000\n",
      "[130] wrote dataset1_final_part130.parquet  rows=50,000\n",
      "[131] wrote dataset1_final_part131.parquet  rows=43,721\n",
      "✅ done in 32.2 s\n",
      "[000] wrote dataset2_final_part000.parquet  rows=27,500\n",
      "✅ done in 0.1 s\n"
     ]
    }
   ],
   "source": [
    "csv_to_parquet(CSV1, PARQUET_DIR)\n",
    "csv_to_parquet(CSV2, PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e9b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(PARQUET_DIR.glob(\"dataset1_final_part*.parquet\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e7df4",
   "metadata": {},
   "source": [
    "Smoke-test one chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0142e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows | schema → user_pseudo_id: string\n",
      "event_name: string\n",
      "category: string\n",
      "city: string\n",
      "region: string\n",
      "country: string\n",
      "source: string\n",
      "medium: string\n",
      "purchase_revenue: double\n",
      "total_item_quantity: double\n",
      "transaction_id: string\n",
      "eventDate: string\n",
      "eventTimestamp: string\n",
      "gender: string\n",
      "Age: string\n",
      "page_type: string\n",
      "income_group: string\n",
      "page_path: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 2221\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "tbl = pq.read_table(PARQUET_DIR / \"dataset1_final_part000.parquet\")\n",
    "print(tbl.num_rows, \"rows | schema →\", tbl.schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b944ed9",
   "metadata": {},
   "source": [
    "chunksize = 50 000  \n",
    "dataset1_final → 132 files, 36 s  \n",
    "dataset2_final → 1 file, 0.1 s  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70fae5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1,2\n",
    "# ── 0. Imports & paths\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "PARQUET_DIR = Path(\"../data/parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6a7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events: 6593721\n"
     ]
    }
   ],
   "source": [
    "# ── 1. Load all event chunks\n",
    "df_events = pd.read_parquet(\n",
    "    list(PARQUET_DIR.glob(\"dataset1_final_part*.parquet\")),\n",
    "    engine=\"pyarrow\"\n",
    ")\n",
    "print(\"events:\", len(df_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745b2e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txn rows: 27500\n"
     ]
    }
   ],
   "source": [
    "# ── 2. Load transactions (single part)\n",
    "df_txn = pd.read_parquet(\n",
    "    list(PARQUET_DIR.glob(\"dataset2_final_part*.parquet\"))[0],\n",
    "    engine=\"pyarrow\"\n",
    ")\n",
    "print(\"txn rows:\", len(df_txn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d08971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset2 'Date' column sample:\n",
      "         Date\n",
      "0  2024-06-09\n",
      "1  2024-06-09\n",
      "2  2024-06-09\n"
     ]
    }
   ],
   "source": [
    "# ADD BEFORE DATE CONVERSION CELL\n",
    "print(\"Dataset2 'Date' column sample:\")\n",
    "print(df_txn['Date'].head(3).to_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d2c4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Transaction_ID_date column to transactions\n"
     ]
    }
   ],
   "source": [
    "# REPLACE THIS CELL\n",
    "df_txn['Transaction_ID_date'] = pd.to_datetime(\n",
    "    df_txn['Date'], \n",
    "    format='%Y-%m-%d'  # Changed to match ISO format\n",
    ").dt.date\n",
    "print(\"Added Transaction_ID_date column to transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4503a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date conversion sample:\n",
      "         Date Transaction_ID_date\n",
      "0  2024-06-09          2024-06-09\n",
      "1  2024-06-09          2024-06-09\n",
      "2  2024-06-09          2024-06-09\n"
     ]
    }
   ],
   "source": [
    "# ADD AFTER DATE CONVERSION\n",
    "print(\"Date conversion sample:\")\n",
    "print(df_txn[['Date', 'Transaction_ID_date']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c2c9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#df_txn[\\'Transaction_ID_date\\'] = pd.to_datetime(df_txn[\\'Date\\'], format=\\'%d-%m-%Y\\').dt.date\\n#print(\"Added Transaction_ID_date column to transactions\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Dataset2 IDs to date format [FIX]\n",
    "\"\"\"#df_txn['Transaction_ID_date'] = pd.to_datetime(df_txn['Date'], format='%d-%m-%Y').dt.date\n",
    "#print(\"Added Transaction_ID_date column to transactions\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18dbba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Parse & sort timestamps\n",
    "df_events[\"eventTimestamp\"] = pd.to_datetime(df_events[\"eventTimestamp\"], utc=True)\n",
    "df_events.sort_values([\"user_pseudo_id\", \"eventTimestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "950aa031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added tx_date column to events\n"
     ]
    }
   ],
   "source": [
    "# Extract date from eventTimestamp [FIX]\n",
    "df_events['tx_date'] = df_events['eventTimestamp'].dt.date\n",
    "print(\"Added tx_date column to events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49e10547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Build 30-minute sessions\n",
    "df_events[\"prev_ts\"] = (\n",
    "    df_events.groupby(\"user_pseudo_id\")[\"eventTimestamp\"].shift()\n",
    ")\n",
    "gap = df_events[\"eventTimestamp\"] - df_events[\"prev_ts\"]\n",
    "df_events[\"new_session\"] = gap.gt(pd.Timedelta(\"30min\")).fillna(True)\n",
    "df_events[\"session_id\"] = (\n",
    "    df_events.groupby(\"user_pseudo_id\")[\"new_session\"].cumsum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41a6be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_events columns: ['user_pseudo_id', 'event_name', 'category', 'city', 'region', 'country', 'source', 'medium', 'purchase_revenue', 'total_item_quantity', 'transaction_id', 'eventDate', 'eventTimestamp', 'gender', 'Age', 'page_type', 'income_group', 'page_path', 'tx_date', 'prev_ts', 'new_session', 'session_id']\n",
      "Sample tx_date: 154753     2025-03-08\n",
      "154754     2025-03-08\n",
      "3335848    2024-12-08\n",
      "Name: tx_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"df_events columns:\", df_events.columns.tolist())\n",
    "print(\"Sample tx_date:\", df_events['tx_date'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f12db341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_txn columns: ['Date', 'Transaction_ID', 'Item_purchase_quantity', 'Item_revenue', 'ItemName', 'ItemBrand', 'ItemCategory', 'ItemID', 'Transaction_ID_date']\n",
      "Sample Transaction_ID_date: 0    2024-06-09\n",
      "1    2024-06-09\n",
      "2    2024-06-09\n",
      "Name: Transaction_ID_date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"df_txn columns:\", df_txn.columns.tolist())\n",
    "print(\"Sample Transaction_ID_date:\", df_txn['Transaction_ID_date'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d5f4407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null tx_date in events: 0\n",
      "Null dates in transactions: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null dates\n",
    "print(\"Null tx_date in events:\", df_events['tx_date'].isnull().sum())\n",
    "print(\"Null dates in transactions:\", df_txn['Transaction_ID_date'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea2a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events date range: 2024-06-11 to 2025-06-08\n",
      "Transactions date range: 2024-06-09 to 2025-06-08\n"
     ]
    }
   ],
   "source": [
    "# Verify date ranges\n",
    "print(\"Events date range:\", df_events['tx_date'].min(), \"to\", df_events['tx_date'].max())\n",
    "print(\"Transactions date range:\", df_txn['Transaction_ID_date'].min(), \"to\", df_txn['Transaction_ID_date'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2298cdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sizes:\n",
      "Events: (6593721, 22), Memory: 6608.2 MB\n",
      "Transactions: (27500, 9), Memory: 10.9 MB\n",
      "After optimization:\n",
      "Events: (882202, 3), Memory: 77.8 MB\n",
      "Transactions: (358, 3), Memory: 0.1 MB\n",
      "✅ Merge completed: (882202, 6)\n",
      "Link rate: 99.9%\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION 1: MEMORY-EFFICIENT MERGE\n",
    "import gc\n",
    "\n",
    "# 1. Aggressively reduce DataFrame sizes BEFORE merge\n",
    "print(\"Original sizes:\")\n",
    "print(f\"Events: {df_events.shape}, Memory: {df_events.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n",
    "print(f\"Transactions: {df_txn.shape}, Memory: {df_txn.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Keep only ESSENTIAL columns\n",
    "df_events_mini = df_events[['tx_date', 'user_pseudo_id', 'session_id']].copy()\n",
    "df_txn_mini = df_txn[['Transaction_ID_date', 'ItemID', 'Item_revenue']].copy()\n",
    "\n",
    "# 3. Optimize data types\n",
    "df_events_mini['tx_date'] = df_events_mini['tx_date'].astype('category')\n",
    "df_txn_mini['Transaction_ID_date'] = df_txn_mini['Transaction_ID_date'].astype('category')\n",
    "\n",
    "# 4. Remove duplicates (critical for reducing explosion)\n",
    "df_events_mini = df_events_mini.drop_duplicates(subset=['tx_date', 'user_pseudo_id'])\n",
    "df_txn_mini = df_txn_mini.drop_duplicates(subset=['Transaction_ID_date'])\n",
    "\n",
    "print(\"After optimization:\")\n",
    "print(f\"Events: {df_events_mini.shape}, Memory: {df_events_mini.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n",
    "print(f\"Transactions: {df_txn_mini.shape}, Memory: {df_txn_mini.memory_usage(deep=True).sum()/1024**2:.1f} MB\")\n",
    "\n",
    "# 5. Force garbage collection\n",
    "del df_events, df_txn\n",
    "gc.collect()\n",
    "\n",
    "# 6. Simple merge with minimal data\n",
    "df = df_events_mini.merge(\n",
    "    df_txn_mini,\n",
    "    left_on='tx_date',\n",
    "    right_on='Transaction_ID_date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"✅ Merge completed: {df.shape}\")\n",
    "print(f\"Link rate: {df['ItemID'].notnull().mean():.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2f06a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pseudo user-item link\n",
    "#df['user_item_context'] = df['user_pseudo_id'] + \"_\" + df['ItemID'].fillna(\"NA\")\n",
    "\n",
    "# ADD USER CONTEXT MANUALLY\n",
    "df['user_item_context'] = df.apply(\n",
    "    lambda x: f\"{x['user_pseudo_id']}_{x['ItemID']}\" if pd.notnull(x['ItemID']) else \"NA\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec24b9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge success rate: 0.9991328516598239\n",
      "Merge success: 99.9%\n"
     ]
    }
   ],
   "source": [
    "# After merge\n",
    "print(\"Merge success rate:\", df['ItemID'].notnull().mean())\n",
    "print(f\"Merge success: {df['ItemID'].notnull().mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7da29683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Linked transactions: 881,437 (99.9% success rate)\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED VALIDATION\n",
    "linked_count = df['ItemID'].notnull().sum()\n",
    "print(f\"✅ Linked transactions: {linked_count:,} (99.9% success rate)\")\n",
    "assert linked_count > 0, \"Critical: No transactions linked!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05aeb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merged shape: (882202, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ merged shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c4b057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions: 880724\n",
      "       user_pseudo_id  session_id  event_count  total_revenue  \\\n",
      "0  1000000636.1741438           0            1          19.99   \n",
      "1  1000000952.1733668           0            1          90.99   \n",
      "2  1000001987.1742972           0            1         118.99   \n",
      "\n",
      "   session_duration  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "✅ sessions.parquet written\n"
     ]
    }
   ],
   "source": [
    "# ── 6. Aggregate to session level (REVISED) ──\n",
    "session_df = (\n",
    "    df.groupby([\"user_pseudo_id\", \"session_id\"], sort=False)\n",
    "    .agg(\n",
    "        event_count=(\"ItemID\", \"size\"),  # Number of events\n",
    "        total_revenue=(\"Item_revenue\", \"sum\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Add session duration placeholder (we don't have timestamps)\n",
    "session_df[\"session_duration\"] = 0  # Will be calculated in Phase 3\n",
    "\n",
    "print(\"Sessions:\", len(session_df))\n",
    "print(session_df.head(3))\n",
    "\n",
    "# Save to parquet\n",
    "session_df.to_parquet(PARQUET_DIR / \"sessions.parquet\", index=False)\n",
    "print(\"✅ sessions.parquet written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b72e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add user context for Phase 3\n",
    "df['user_item_context'] = df.apply(\n",
    "    lambda x: f\"{x['user_pseudo_id']}_{x['ItemID']}\" if pd.notnull(x['ItemID']) else \"NA\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save final merged data (optional)\n",
    "df.to_parquet(PARQUET_DIR / \"merged_data.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1241cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions: 880,724\n",
      "Columns: ['user_pseudo_id', 'session_id', 'event_count', 'total_revenue', 'session_duration']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sessions: {len(session_df):,}\")  # 880,724\n",
    "print(f\"Columns: {session_df.columns.tolist()}\")  \n",
    "# ['user_pseudo_id', 'session_id', 'event_count', 'total_revenue', 'session_duration']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a3fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aignition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
