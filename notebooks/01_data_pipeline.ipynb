{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 – metadata\n",
    "# Direct Pandas Pipeline – chunked CSV → Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c8ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 – imports & config\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "CSV1 = Path(\"../data/raw/dataset1_final.csv\")\n",
    "CSV2 = Path(\"../data/raw/dataset2_final.csv\")\n",
    "CHUNK = 50_000\n",
    "PARQUET_DIR = Path(\"../data/parquet\")\n",
    "PARQUET_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fbd66df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>category</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>source</th>\n",
       "      <th>medium</th>\n",
       "      <th>purchase_revenue</th>\n",
       "      <th>total_item_quantity</th>\n",
       "      <th>transaction_id</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>eventTimestamp</th>\n",
       "      <th>gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>page_type</th>\n",
       "      <th>income_group</th>\n",
       "      <th>page_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.789251e+09</td>\n",
       "      <td>session_start</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Poquoson</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 10:21:57.850268</td>\n",
       "      <td>male</td>\n",
       "      <td>35-44</td>\n",
       "      <td>homepage</td>\n",
       "      <td>Top 10%</td>\n",
       "      <td>https://demo.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.789251e+09</td>\n",
       "      <td>page_view</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Poquoson</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 10:21:57.850268</td>\n",
       "      <td>female</td>\n",
       "      <td>above 64</td>\n",
       "      <td>homepage</td>\n",
       "      <td>below 50%</td>\n",
       "      <td>https://demo.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.788384e+09</td>\n",
       "      <td>session_start</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Carthage</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 12:38:06.968220</td>\n",
       "      <td>male</td>\n",
       "      <td>45-54</td>\n",
       "      <td>collections</td>\n",
       "      <td>11-20%</td>\n",
       "      <td>https://demo.com/collections/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.788384e+09</td>\n",
       "      <td>page_view</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Carthage</td>\n",
       "      <td>New York</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>PaidSocial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 12:38:06.968220</td>\n",
       "      <td>male</td>\n",
       "      <td>45-54</td>\n",
       "      <td>collections</td>\n",
       "      <td>11-20%</td>\n",
       "      <td>https://demo.com/collections/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.198796e+09</td>\n",
       "      <td>page_view</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>United States</td>\n",
       "      <td>(direct)</td>\n",
       "      <td>(none)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-13</td>\n",
       "      <td>2025-05-13 14:20:32.933828</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34</td>\n",
       "      <td>products</td>\n",
       "      <td>Top 10%</td>\n",
       "      <td>https://demo.com/products/ITEM377/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_pseudo_id     event_name category      city    region        country  \\\n",
       "0    1.789251e+09  session_start   mobile  Poquoson  Virginia  United States   \n",
       "1    1.789251e+09      page_view   mobile  Poquoson  Virginia  United States   \n",
       "2    1.788384e+09  session_start   mobile  Carthage  New York  United States   \n",
       "3    1.788384e+09      page_view   mobile  Carthage  New York  United States   \n",
       "4    1.198796e+09      page_view   mobile   Phoenix   Arizona  United States   \n",
       "\n",
       "     source      medium  purchase_revenue  total_item_quantity  \\\n",
       "0  Facebook  PaidSocial               NaN                  NaN   \n",
       "1  Facebook  PaidSocial               NaN                  NaN   \n",
       "2  Facebook  PaidSocial               NaN                  NaN   \n",
       "3  Facebook  PaidSocial               NaN                  NaN   \n",
       "4  (direct)      (none)               NaN                  NaN   \n",
       "\n",
       "   transaction_id   eventDate              eventTimestamp  gender       Age  \\\n",
       "0             NaN  2025-05-13  2025-05-13 10:21:57.850268    male     35-44   \n",
       "1             NaN  2025-05-13  2025-05-13 10:21:57.850268  female  above 64   \n",
       "2             NaN  2025-05-13  2025-05-13 12:38:06.968220    male     45-54   \n",
       "3             NaN  2025-05-13  2025-05-13 12:38:06.968220    male     45-54   \n",
       "4             NaN  2025-05-13  2025-05-13 14:20:32.933828    male     25-34   \n",
       "\n",
       "     page_type income_group                           page_path  \n",
       "0     homepage      Top 10%                   https://demo.com/  \n",
       "1     homepage    below 50%                   https://demo.com/  \n",
       "2  collections       11-20%       https://demo.com/collections/  \n",
       "3  collections       11-20%       https://demo.com/collections/  \n",
       "4     products      Top 10%  https://demo.com/products/ITEM377/  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(CSV1, nrows=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17696f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(src: Path, dst_dir: Path, chunksize: int = 50_000):\n",
    "    \"\"\"Read a large CSV in chunks and write each chunk to Parquet.\"\"\"\n",
    "    import time, pandas as pd\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(src, chunksize=chunksize, dtype_backend=\"pyarrow\")):\n",
    "        outfile = dst_dir / f\"{src.stem}_part{i:03d}.parquet\"\n",
    "        chunk.to_parquet(outfile, engine=\"pyarrow\", index=False)\n",
    "        print(f\"[{i:03d}] wrote {outfile.name}  rows={len(chunk):,}\")\n",
    "\n",
    "    print(\"✅ done in\", round(time.time() - t0, 1), \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97727639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] wrote dataset1_final_part000.parquet  rows=50,000\n",
      "[001] wrote dataset1_final_part001.parquet  rows=50,000\n",
      "[002] wrote dataset1_final_part002.parquet  rows=50,000\n",
      "[003] wrote dataset1_final_part003.parquet  rows=50,000\n",
      "[004] wrote dataset1_final_part004.parquet  rows=50,000\n",
      "[005] wrote dataset1_final_part005.parquet  rows=50,000\n",
      "[006] wrote dataset1_final_part006.parquet  rows=50,000\n",
      "[007] wrote dataset1_final_part007.parquet  rows=50,000\n",
      "[008] wrote dataset1_final_part008.parquet  rows=50,000\n",
      "[009] wrote dataset1_final_part009.parquet  rows=50,000\n",
      "[010] wrote dataset1_final_part010.parquet  rows=50,000\n",
      "[011] wrote dataset1_final_part011.parquet  rows=50,000\n",
      "[012] wrote dataset1_final_part012.parquet  rows=50,000\n",
      "[013] wrote dataset1_final_part013.parquet  rows=50,000\n",
      "[014] wrote dataset1_final_part014.parquet  rows=50,000\n",
      "[015] wrote dataset1_final_part015.parquet  rows=50,000\n",
      "[016] wrote dataset1_final_part016.parquet  rows=50,000\n",
      "[017] wrote dataset1_final_part017.parquet  rows=50,000\n",
      "[018] wrote dataset1_final_part018.parquet  rows=50,000\n",
      "[019] wrote dataset1_final_part019.parquet  rows=50,000\n",
      "[020] wrote dataset1_final_part020.parquet  rows=50,000\n",
      "[021] wrote dataset1_final_part021.parquet  rows=50,000\n",
      "[022] wrote dataset1_final_part022.parquet  rows=50,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koppu\\AppData\\Local\\Temp\\ipykernel_2820\\2611443359.py:6: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv(src, chunksize=chunksize, dtype_backend=\"pyarrow\")):\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert '452362343.1719424151' with type str: tried to convert to double\", 'Conversion failed for column user_pseudo_id with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# convert both datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcsv_to_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPARQUET_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m csv_to_parquet(CSV2, PARQUET_DIR)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mcsv_to_parquet\u001b[1;34m(src, dst_dir, chunksize)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(src, chunksize\u001b[38;5;241m=\u001b[39mchunksize, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m      7\u001b[0m     outfile \u001b[38;5;241m=\u001b[39m dst_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_part\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpyarrow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutfile\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  rows=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ done in\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pandas\\core\\frame.py:3118\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3038\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[0;32m   3039\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3114\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[0;32m   3115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 3118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_parquet(\n\u001b[0;32m   3119\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3120\u001b[0m     path,\n\u001b[0;32m   3121\u001b[0m     engine,\n\u001b[0;32m   3122\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3123\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   3124\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m   3125\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3126\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3127\u001b[0m )\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pandas\\io\\parquet.py:482\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[0;32m    480\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[1;32m--> 482\u001b[0m impl\u001b[38;5;241m.\u001b[39mwrite(\n\u001b[0;32m    483\u001b[0m     df,\n\u001b[0;32m    484\u001b[0m     path_or_buf,\n\u001b[0;32m    485\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    486\u001b[0m     index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m    487\u001b[0m     partition_cols\u001b[38;5;241m=\u001b[39mpartition_cols,\n\u001b[0;32m    488\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    489\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pandas\\io\\parquet.py:191\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m     from_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreserve_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m index\n\u001b[1;32m--> 191\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_pandas(df, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pandas_kwargs)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[0;32m    194\u001b[0m     df_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANDAS_ATTRS\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(df\u001b[38;5;241m.\u001b[39mattrs)}\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\table.pxi:4793\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\pandas_compat.py:652\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, maybe_fut \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arrays):\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_fut, futures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m--> 652\u001b[0m             arrays[i] \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_fut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    654\u001b[0m types \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\pandas_compat.py:626\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[0;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    623\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    624\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m field_nullable \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mnull_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mField \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was non-nullable but pandas column \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhad \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m null values\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(field),\n\u001b[0;32m    630\u001b[0m                                                  result\u001b[38;5;241m.\u001b[39mnull_count))\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\pandas_compat.py:620\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[1;34m(col, field)\u001b[0m\n\u001b[0;32m    617\u001b[0m     type_ \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 620\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid,\n\u001b[0;32m    622\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowNotImplementedError,\n\u001b[0;32m    623\u001b[0m         pa\u001b[38;5;241m.\u001b[39mArrowTypeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    624\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion failed for column \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m with type \u001b[39m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    625\u001b[0m                \u001b[38;5;241m.\u001b[39mformat(col\u001b[38;5;241m.\u001b[39mname, col\u001b[38;5;241m.\u001b[39mdtype),)\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\array.pxi:365\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\array.pxi:90\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\aignition\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: (\"Could not convert '452362343.1719424151' with type str: tried to convert to double\", 'Conversion failed for column user_pseudo_id with type object')"
     ]
    }
   ],
   "source": [
    "# convert both datasets\n",
    "csv_to_parquet(CSV1, PARQUET_DIR)\n",
    "csv_to_parquet(CSV2, PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fb586b",
   "metadata": {},
   "source": [
    "chunksize:        50 000\n",
    "parquet_engine:   pyarrow\n",
    "user_pseudo_id:   forced string  (avoid ArrowInvalid on mixed types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b52f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(src: Path, dst_dir: Path, chunksize: int = 50_000):\n",
    "    \"\"\"Read a large CSV in chunks and write each chunk to Parquet.\"\"\"\n",
    "    import time, pandas as pd\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ↓ Explicit dtype prevents mixed-type surprises\n",
    "    explicit = {\"user_pseudo_id\": \"string\"}      # keep IDs as strings\n",
    "    for i, chunk in enumerate(\n",
    "        pd.read_csv(src,\n",
    "                    chunksize=chunksize,\n",
    "                    dtype=explicit,          # <- key change\n",
    "                    low_memory=False)\n",
    "    ):\n",
    "        # (optional) drop the .000 / scientific notation if you prefer integers\n",
    "        # chunk[\"user_pseudo_id\"] = (\n",
    "        #     pd.to_numeric(chunk[\"user_pseudo_id\"], errors=\"coerce\")\n",
    "        #       .astype(\"Int64\")  # nullable int\n",
    "        # )\n",
    "\n",
    "        outfile = dst_dir / f\"{src.stem}_part{i:03d}.parquet\"\n",
    "        chunk.to_parquet(outfile, engine=\"pyarrow\", index=False)\n",
    "        print(f\"[{i:03d}] wrote {outfile.name}  rows={len(chunk):,}\")\n",
    "\n",
    "    print(\"✅ done in\", round(time.time() - t0, 1), \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc378edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] wrote dataset1_final_part000.parquet  rows=50,000\n",
      "[001] wrote dataset1_final_part001.parquet  rows=50,000\n",
      "[002] wrote dataset1_final_part002.parquet  rows=50,000\n",
      "[003] wrote dataset1_final_part003.parquet  rows=50,000\n",
      "[004] wrote dataset1_final_part004.parquet  rows=50,000\n",
      "[005] wrote dataset1_final_part005.parquet  rows=50,000\n",
      "[006] wrote dataset1_final_part006.parquet  rows=50,000\n",
      "[007] wrote dataset1_final_part007.parquet  rows=50,000\n",
      "[008] wrote dataset1_final_part008.parquet  rows=50,000\n",
      "[009] wrote dataset1_final_part009.parquet  rows=50,000\n",
      "[010] wrote dataset1_final_part010.parquet  rows=50,000\n",
      "[011] wrote dataset1_final_part011.parquet  rows=50,000\n",
      "[012] wrote dataset1_final_part012.parquet  rows=50,000\n",
      "[013] wrote dataset1_final_part013.parquet  rows=50,000\n",
      "[014] wrote dataset1_final_part014.parquet  rows=50,000\n",
      "[015] wrote dataset1_final_part015.parquet  rows=50,000\n",
      "[016] wrote dataset1_final_part016.parquet  rows=50,000\n",
      "[017] wrote dataset1_final_part017.parquet  rows=50,000\n",
      "[018] wrote dataset1_final_part018.parquet  rows=50,000\n",
      "[019] wrote dataset1_final_part019.parquet  rows=50,000\n",
      "[020] wrote dataset1_final_part020.parquet  rows=50,000\n",
      "[021] wrote dataset1_final_part021.parquet  rows=50,000\n",
      "[022] wrote dataset1_final_part022.parquet  rows=50,000\n",
      "[023] wrote dataset1_final_part023.parquet  rows=50,000\n",
      "[024] wrote dataset1_final_part024.parquet  rows=50,000\n",
      "[025] wrote dataset1_final_part025.parquet  rows=50,000\n",
      "[026] wrote dataset1_final_part026.parquet  rows=50,000\n",
      "[027] wrote dataset1_final_part027.parquet  rows=50,000\n",
      "[028] wrote dataset1_final_part028.parquet  rows=50,000\n",
      "[029] wrote dataset1_final_part029.parquet  rows=50,000\n",
      "[030] wrote dataset1_final_part030.parquet  rows=50,000\n",
      "[031] wrote dataset1_final_part031.parquet  rows=50,000\n",
      "[032] wrote dataset1_final_part032.parquet  rows=50,000\n",
      "[033] wrote dataset1_final_part033.parquet  rows=50,000\n",
      "[034] wrote dataset1_final_part034.parquet  rows=50,000\n",
      "[035] wrote dataset1_final_part035.parquet  rows=50,000\n",
      "[036] wrote dataset1_final_part036.parquet  rows=50,000\n",
      "[037] wrote dataset1_final_part037.parquet  rows=50,000\n",
      "[038] wrote dataset1_final_part038.parquet  rows=50,000\n",
      "[039] wrote dataset1_final_part039.parquet  rows=50,000\n",
      "[040] wrote dataset1_final_part040.parquet  rows=50,000\n",
      "[041] wrote dataset1_final_part041.parquet  rows=50,000\n",
      "[042] wrote dataset1_final_part042.parquet  rows=50,000\n",
      "[043] wrote dataset1_final_part043.parquet  rows=50,000\n",
      "[044] wrote dataset1_final_part044.parquet  rows=50,000\n",
      "[045] wrote dataset1_final_part045.parquet  rows=50,000\n",
      "[046] wrote dataset1_final_part046.parquet  rows=50,000\n",
      "[047] wrote dataset1_final_part047.parquet  rows=50,000\n",
      "[048] wrote dataset1_final_part048.parquet  rows=50,000\n",
      "[049] wrote dataset1_final_part049.parquet  rows=50,000\n",
      "[050] wrote dataset1_final_part050.parquet  rows=50,000\n",
      "[051] wrote dataset1_final_part051.parquet  rows=50,000\n",
      "[052] wrote dataset1_final_part052.parquet  rows=50,000\n",
      "[053] wrote dataset1_final_part053.parquet  rows=50,000\n",
      "[054] wrote dataset1_final_part054.parquet  rows=50,000\n",
      "[055] wrote dataset1_final_part055.parquet  rows=50,000\n",
      "[056] wrote dataset1_final_part056.parquet  rows=50,000\n",
      "[057] wrote dataset1_final_part057.parquet  rows=50,000\n",
      "[058] wrote dataset1_final_part058.parquet  rows=50,000\n",
      "[059] wrote dataset1_final_part059.parquet  rows=50,000\n",
      "[060] wrote dataset1_final_part060.parquet  rows=50,000\n",
      "[061] wrote dataset1_final_part061.parquet  rows=50,000\n",
      "[062] wrote dataset1_final_part062.parquet  rows=50,000\n",
      "[063] wrote dataset1_final_part063.parquet  rows=50,000\n",
      "[064] wrote dataset1_final_part064.parquet  rows=50,000\n",
      "[065] wrote dataset1_final_part065.parquet  rows=50,000\n",
      "[066] wrote dataset1_final_part066.parquet  rows=50,000\n",
      "[067] wrote dataset1_final_part067.parquet  rows=50,000\n",
      "[068] wrote dataset1_final_part068.parquet  rows=50,000\n",
      "[069] wrote dataset1_final_part069.parquet  rows=50,000\n",
      "[070] wrote dataset1_final_part070.parquet  rows=50,000\n",
      "[071] wrote dataset1_final_part071.parquet  rows=50,000\n",
      "[072] wrote dataset1_final_part072.parquet  rows=50,000\n",
      "[073] wrote dataset1_final_part073.parquet  rows=50,000\n",
      "[074] wrote dataset1_final_part074.parquet  rows=50,000\n",
      "[075] wrote dataset1_final_part075.parquet  rows=50,000\n",
      "[076] wrote dataset1_final_part076.parquet  rows=50,000\n",
      "[077] wrote dataset1_final_part077.parquet  rows=50,000\n",
      "[078] wrote dataset1_final_part078.parquet  rows=50,000\n",
      "[079] wrote dataset1_final_part079.parquet  rows=50,000\n",
      "[080] wrote dataset1_final_part080.parquet  rows=50,000\n",
      "[081] wrote dataset1_final_part081.parquet  rows=50,000\n",
      "[082] wrote dataset1_final_part082.parquet  rows=50,000\n",
      "[083] wrote dataset1_final_part083.parquet  rows=50,000\n",
      "[084] wrote dataset1_final_part084.parquet  rows=50,000\n",
      "[085] wrote dataset1_final_part085.parquet  rows=50,000\n",
      "[086] wrote dataset1_final_part086.parquet  rows=50,000\n",
      "[087] wrote dataset1_final_part087.parquet  rows=50,000\n",
      "[088] wrote dataset1_final_part088.parquet  rows=50,000\n",
      "[089] wrote dataset1_final_part089.parquet  rows=50,000\n",
      "[090] wrote dataset1_final_part090.parquet  rows=50,000\n",
      "[091] wrote dataset1_final_part091.parquet  rows=50,000\n",
      "[092] wrote dataset1_final_part092.parquet  rows=50,000\n",
      "[093] wrote dataset1_final_part093.parquet  rows=50,000\n",
      "[094] wrote dataset1_final_part094.parquet  rows=50,000\n",
      "[095] wrote dataset1_final_part095.parquet  rows=50,000\n",
      "[096] wrote dataset1_final_part096.parquet  rows=50,000\n",
      "[097] wrote dataset1_final_part097.parquet  rows=50,000\n",
      "[098] wrote dataset1_final_part098.parquet  rows=50,000\n",
      "[099] wrote dataset1_final_part099.parquet  rows=50,000\n",
      "[100] wrote dataset1_final_part100.parquet  rows=50,000\n",
      "[101] wrote dataset1_final_part101.parquet  rows=50,000\n",
      "[102] wrote dataset1_final_part102.parquet  rows=50,000\n",
      "[103] wrote dataset1_final_part103.parquet  rows=50,000\n",
      "[104] wrote dataset1_final_part104.parquet  rows=50,000\n",
      "[105] wrote dataset1_final_part105.parquet  rows=50,000\n",
      "[106] wrote dataset1_final_part106.parquet  rows=50,000\n",
      "[107] wrote dataset1_final_part107.parquet  rows=50,000\n",
      "[108] wrote dataset1_final_part108.parquet  rows=50,000\n",
      "[109] wrote dataset1_final_part109.parquet  rows=50,000\n",
      "[110] wrote dataset1_final_part110.parquet  rows=50,000\n",
      "[111] wrote dataset1_final_part111.parquet  rows=50,000\n",
      "[112] wrote dataset1_final_part112.parquet  rows=50,000\n",
      "[113] wrote dataset1_final_part113.parquet  rows=50,000\n",
      "[114] wrote dataset1_final_part114.parquet  rows=50,000\n",
      "[115] wrote dataset1_final_part115.parquet  rows=50,000\n",
      "[116] wrote dataset1_final_part116.parquet  rows=50,000\n",
      "[117] wrote dataset1_final_part117.parquet  rows=50,000\n",
      "[118] wrote dataset1_final_part118.parquet  rows=50,000\n",
      "[119] wrote dataset1_final_part119.parquet  rows=50,000\n",
      "[120] wrote dataset1_final_part120.parquet  rows=50,000\n",
      "[121] wrote dataset1_final_part121.parquet  rows=50,000\n",
      "[122] wrote dataset1_final_part122.parquet  rows=50,000\n",
      "[123] wrote dataset1_final_part123.parquet  rows=50,000\n",
      "[124] wrote dataset1_final_part124.parquet  rows=50,000\n",
      "[125] wrote dataset1_final_part125.parquet  rows=50,000\n",
      "[126] wrote dataset1_final_part126.parquet  rows=50,000\n",
      "[127] wrote dataset1_final_part127.parquet  rows=50,000\n",
      "[128] wrote dataset1_final_part128.parquet  rows=50,000\n",
      "[129] wrote dataset1_final_part129.parquet  rows=50,000\n",
      "[130] wrote dataset1_final_part130.parquet  rows=50,000\n",
      "[131] wrote dataset1_final_part131.parquet  rows=43,721\n",
      "✅ done in 36.0 s\n",
      "[000] wrote dataset2_final_part000.parquet  rows=27,500\n",
      "✅ done in 0.1 s\n"
     ]
    }
   ],
   "source": [
    "csv_to_parquet(CSV1, PARQUET_DIR)\n",
    "csv_to_parquet(CSV2, PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e9b5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(PARQUET_DIR.glob(\"dataset1_final_part*.parquet\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e7df4",
   "metadata": {},
   "source": [
    "Smoke-test one chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0142e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 rows | schema → user_pseudo_id: string\n",
      "event_name: string\n",
      "category: string\n",
      "city: string\n",
      "region: string\n",
      "country: string\n",
      "source: string\n",
      "medium: string\n",
      "purchase_revenue: double\n",
      "total_item_quantity: double\n",
      "transaction_id: string\n",
      "eventDate: string\n",
      "eventTimestamp: string\n",
      "gender: string\n",
      "Age: string\n",
      "page_type: string\n",
      "income_group: string\n",
      "page_path: string\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 2221\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "tbl = pq.read_table(PARQUET_DIR / \"dataset1_final_part000.parquet\")\n",
    "print(tbl.num_rows, \"rows | schema →\", tbl.schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b944ed9",
   "metadata": {},
   "source": [
    "chunksize = 50 000  \n",
    "dataset1_final → 132 files, 36 s  \n",
    "dataset2_final → 1 file, 0.1 s  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fae5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1,2\n",
    "# ── 0. Imports & paths\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "PARQUET_DIR = Path(\"../data/parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b6a7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events: 6593721\n"
     ]
    }
   ],
   "source": [
    "# ── 1. Load all event chunks\n",
    "df_events = pd.read_parquet(\n",
    "    list(PARQUET_DIR.glob(\"dataset1_final_part*.parquet\")),\n",
    "    engine=\"pyarrow\"\n",
    ")\n",
    "print(\"events:\", len(df_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745b2e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txn rows: 27500\n"
     ]
    }
   ],
   "source": [
    "# ── 2. Load transactions (single part)\n",
    "df_txn = pd.read_parquet(\n",
    "    list(PARQUET_DIR.glob(\"dataset2_final_part*.parquet\"))[0],\n",
    "    engine=\"pyarrow\"\n",
    ")\n",
    "print(\"txn rows:\", len(df_txn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18dbba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. Parse & sort timestamps\n",
    "df_events[\"eventTimestamp\"] = pd.to_datetime(df_events[\"eventTimestamp\"], utc=True)\n",
    "df_events.sort_values([\"user_pseudo_id\", \"eventTimestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49e10547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 4. Build 30-minute sessions\n",
    "df_events[\"prev_ts\"] = (\n",
    "    df_events.groupby(\"user_pseudo_id\")[\"eventTimestamp\"].shift()\n",
    ")\n",
    "gap = df_events[\"eventTimestamp\"] - df_events[\"prev_ts\"]\n",
    "df_events[\"new_session\"] = gap.gt(pd.Timedelta(\"30min\")).fillna(True)\n",
    "df_events[\"session_id\"] = (\n",
    "    df_events.groupby(\"user_pseudo_id\")[\"new_session\"].cumsum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ee1fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 bis – harmonise dtypes before merging\n",
    "df_events[\"transaction_id\"] = df_events[\"transaction_id\"].astype(\"string\")\n",
    "df_txn[\"Transaction_ID\"]    = df_txn[\"Transaction_ID\"].astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9898a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 5. Merge revenue onto events\n",
    "df = df_events.merge(\n",
    "    df_txn,\n",
    "    left_on=\"transaction_id\",\n",
    "    right_on=\"Transaction_ID\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05aeb980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merged shape: (6602845, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ merged shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e9bc460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions: 1004534\n",
      "✅ sessions.parquet written\n"
     ]
    }
   ],
   "source": [
    "# ── 6. Aggregate to session level\n",
    "session_df = (\n",
    "    df.groupby([\"user_pseudo_id\", \"session_id\"], sort=False)\n",
    "      .agg(\n",
    "          page_views=(\"event_name\", \"size\"),\n",
    "          session_start=(\"eventTimestamp\", \"min\"),\n",
    "          session_end=(\"eventTimestamp\", \"max\"),\n",
    "          revenue=(\"purchase_revenue\", \"sum\"),\n",
    "          items=(\"total_item_quantity\", \"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(\"sessions:\", len(session_df))\n",
    "session_df.to_parquet(PARQUET_DIR / \"sessions.parquet\", index=False)\n",
    "print(\"✅ sessions.parquet written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceb57b",
   "metadata": {},
   "source": [
    "Session rule  : new session if time gap > 30 min  \n",
    "Output rows   : 1004534\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75403f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aignition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
