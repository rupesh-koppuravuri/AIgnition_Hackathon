{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53dcfb5e",
   "metadata": {},
   "source": [
    "## Phase 2 – Step 2.3 : Cold-Start Mapping & Fallbacks  \n",
    "**Goal** Map anonymous / first-session users to one of the five K-means segments using only attributes available on the very first page-view (Age, traffic source, country).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d2c732",
   "metadata": {},
   "source": [
    "Imports & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896bbb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features  → (1004534, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, yaml, joblib, pathlib\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT        = pathlib.Path(\"..\")                 # repo root (one level up)\n",
    "PARQUET_DIR = ROOT / \"data\" / \"parquet\"\n",
    "CONFIG_DIR  = ROOT / \"config\"\n",
    "CONFIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# session-level features (already in RAM earlier, but reload for self-containment)\n",
    "FEATS = pd.read_parquet(PARQUET_DIR / \"features.parquet\")\n",
    "print(\"features  →\", FEATS.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48232c",
   "metadata": {},
   "source": [
    " Load event-level columns that carry Age & source, then merge segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d96a89",
   "metadata": {},
   "source": [
    "1 Load event-level columns, attach segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51ef50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events  → (3036407, 4)\n",
      "events+seg → (4958492, 5)\n"
     ]
    }
   ],
   "source": [
    "# ❶  Read only the columns we need from every events chunk\n",
    "event_cols = [\"user_pseudo_id\", \"Age\", \"source\", \"transaction_id\"]\n",
    "events = pd.read_parquet(\n",
    "    list(PARQUET_DIR.glob(\"dataset1_final_part*.parquet\")),\n",
    "    columns=event_cols\n",
    ").drop_duplicates()\n",
    "print(\"events  →\", events.shape)\n",
    "\n",
    "# ❷  Bring in the 5-cluster label we built in Step 2.2\n",
    "seg_map = pd.read_parquet(PARQUET_DIR / \"segment_map.parquet\")\n",
    "events = events.merge(seg_map, on=\"user_pseudo_id\", how=\"inner\")\n",
    "print(\"events+seg →\", events.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6145e",
   "metadata": {},
   "source": [
    "2 Majority-vote rules (Age × source → segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d3a714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct rules: 3174\n",
      "✅  segment_fallback.yaml written → config\\segment_fallback.yaml\n"
     ]
    }
   ],
   "source": [
    "rule_tbl = (\n",
    "    events.groupby([\"Age\", \"source\"])[\"segment\"]\n",
    "          .agg(lambda x: x.value_counts().idxmax())\n",
    "          .reset_index()\n",
    ")\n",
    "print(\"distinct rules:\", len(rule_tbl))\n",
    "\n",
    "#  nested dict  →  YAML\n",
    "nested = defaultdict(dict)\n",
    "for _, r in rule_tbl.iterrows():\n",
    "    nested[r[\"Age\"]][r[\"source\"]] = int(r[\"segment\"])\n",
    "\n",
    "yaml.safe_dump(dict(nested),\n",
    "               open(CONFIG_DIR / \"segment_fallback.yaml\", \"w\"))\n",
    "\n",
    "print(\"✅  segment_fallback.yaml written →\", (CONFIG_DIR / \"segment_fallback.yaml\").relative_to(ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd364d",
   "metadata": {},
   "source": [
    "3 Map Transaction_ID to segment (for popular items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b771de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx2seg  → (18262, 2)\n",
      "purch_seg → (27500, 10) |  segment nulls: 150\n"
     ]
    }
   ],
   "source": [
    "# ❶  Build lookup transaction_id  →  segment\n",
    "tx2seg = events[[\"transaction_id\", \"segment\"]].dropna().drop_duplicates()\n",
    "tx2seg[\"transaction_id\"] = tx2seg[\"transaction_id\"].astype(\"string\")\n",
    "\n",
    "print(\"tx2seg  →\", tx2seg.shape)\n",
    "\n",
    "# ❷  Load purchase table and attach segment\n",
    "purch = pd.read_parquet(PARQUET_DIR / \"dataset2_final_part000.parquet\")\n",
    "purch[\"Transaction_ID\"] = purch[\"Transaction_ID\"].astype(\"string\")\n",
    "\n",
    "purch_seg = purch.merge(\n",
    "    tx2seg, left_on=\"Transaction_ID\", right_on=\"transaction_id\", how=\"left\"\n",
    ")\n",
    "print(\"purch_seg →\", purch_seg.shape, \"|  segment nulls:\", purch_seg[\"segment\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f519c",
   "metadata": {},
   "source": [
    "Top-N popular items (overall and per segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b499348",
   "metadata": {},
   "source": [
    "4 Top-30 popular items per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebd27b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  popular_items.parquet written: (1226, 3)\n"
     ]
    }
   ],
   "source": [
    "TOP_N = 30\n",
    "popular_tbl = (\n",
    "    purch_seg.groupby([\"segment\", \"ItemID\"])\n",
    "             .size().reset_index(name=\"qty\")\n",
    "             .sort_values([\"segment\", \"qty\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "popular_tbl.to_parquet(PARQUET_DIR / \"popular_items.parquet\", index=False)\n",
    "print(\"✅  popular_items.parquet written:\", popular_tbl.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179500e",
   "metadata": {},
   "source": [
    "5 (OPTIONAL) tiny decision tree for explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d777fbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- frequency <= 625.50\n",
      "|   |--- recency_days <= 117.50\n",
      "|   |   |--- recency_days <= 100.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- recency_days >  100.50\n",
      "|   |   |   |--- class: 1\n",
      "|   |--- recency_days >  117.50\n",
      "|   |   |--- recency_days <= 185.50\n",
      "|   |   |   |--- class: 4\n",
      "|   |   |--- recency_days >  185.50\n",
      "|   |   |   |--- class: 3\n",
      "|--- frequency >  625.50\n",
      "|   |--- class: 2\n",
      "\n",
      "✅  routing_tree.pkl saved\n"
     ]
    }
   ],
   "source": [
    "seg_map = pd.read_parquet(PARQUET_DIR / \"segment_map.parquet\")\n",
    "FEATS_seg = FEATS.merge(seg_map, on=\"user_pseudo_id\", how=\"left\")\n",
    "\n",
    "X = FEATS_seg[[\"recency_days\", \"frequency\", \"monetary_value\"]]\n",
    "y = FEATS_seg[\"segment\"]\n",
    "\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tree = DecisionTreeClassifier(max_depth=3, random_state=42).fit(Xtr, ytr)\n",
    "\n",
    "print(export_text(tree, feature_names=list(X.columns)))\n",
    "\n",
    "joblib.dump(tree, ROOT / \"src\" / \"routing_tree.pkl\")\n",
    "print(\"✅  routing_tree.pkl saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27060cfe",
   "metadata": {},
   "source": [
    "6 Markdown – cold-start flow (add as a Markdown cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4239aeda",
   "metadata": {},
   "source": [
    "### Cold-Start Logic\n",
    "\n",
    "1. First page-view provides **Age** and **traffic source**.  \n",
    "2. `segment_fallback.yaml` maps (Age, source) → one of 5 segments.  \n",
    "   If no match, fall back to global modal segment **{FEATS[\"segment\"].mode()[0]}**.  \n",
    "3. Recommend Top-30 items from `popular_items.parquet` for that segment until the first session ends.  \n",
    "4. After session close, user is re-clustered with the full K-means model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aignition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
